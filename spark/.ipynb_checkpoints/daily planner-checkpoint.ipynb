{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416a4bc-8784-425d-ae45-80e9fa02ad15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## firebase setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78517a12-d753-4a00-b8d2-c465f080e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n",
      "Firebase Admin SDK initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/google/api_core/_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.8.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import os\n",
    "\n",
    "SERVICE_ACCOUNT_KEY_PATH = os.environ.get(\"FIREBASE_SERVICE_ACCOUNT_KEY_PATH\", \"./cstam2-1f2ec-firebase-adminsdk-fbsvc-2ab61a7ed6.json\")\n",
    "\n",
    "try:\n",
    "    # Check if the default app is already initialized\n",
    "    app = firebase_admin.get_app()\n",
    "    print(\"Firebase Admin SDK already initialized. Reusing existing app instance.\")\n",
    "except ValueError:\n",
    "    # If not initialized, proceed with initialization\n",
    "    try:\n",
    "        cred = credentials.Certificate(SERVICE_ACCOUNT_KEY_PATH)\n",
    "        app = firebase_admin.initialize_app(cred)\n",
    "        print(\"Firebase Admin SDK initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Firebase Admin SDK initialization: {e}\")\n",
    "        # It's crucial to handle this error, as your app can't write to Firestore without it.\n",
    "        raise # Re-raise to stop the script if Firebase initialization fails\n",
    "\n",
    "db = firestore.client(app=app) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b4408-f73f-4f3c-a981-fc249d41a402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90671972-b60d-4b63-bc0a-391fab472f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 10:45:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/10 10:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, to_timestamp, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger(\"DataCleaning\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)\n",
    "# ---------- Spark session ----------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"health-streams-to-firebase\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d937a60-f0bb-4b5b-9575-033e4a1b5dc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## daily data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c657d66e-9ee4-416d-a84c-9ef4d68a5ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+---------+---------------+---------+--------+-----------+\n",
      "|user_id   |date      |meal_type|food_name |water_ml |sport_available|weight_kg|height_m|sleep_hours|\n",
      "+----------+----------+---------+----------+---------+---------------+---------+--------+-----------+\n",
      "|5577150313|2025-11-01|Breakfast|Butter    |433.53418|false          |77.4     |1.84    |8.6        |\n",
      "|5577150313|2025-11-01|Lunch    |Pork Chop |618.15173|false          |77.4     |1.84    |8.6        |\n",
      "|5577150313|2025-11-01|Dinner   |Milk      |686.53754|false          |77.4     |1.84    |8.6        |\n",
      "|5577150313|2025-11-01|Snack    |Water     |235.68057|false          |77.4     |1.84    |8.6        |\n",
      "|5577150313|2025-11-02|Breakfast|Water     |369.77222|false          |77.7     |1.84    |4.6        |\n",
      "|5577150313|2025-11-02|Lunch    |Banana    |482.8055 |false          |77.7     |1.84    |4.6        |\n",
      "|5577150313|2025-11-02|Dinner   |Beef Steak|682.02374|false          |77.7     |1.84    |4.6        |\n",
      "|5577150313|2025-11-02|Snack    |Green Tea |162.28773|false          |77.7     |1.84    |4.6        |\n",
      "|5577150313|2025-11-03|Breakfast|Rice      |390.26404|false          |78.0     |1.84    |6.4        |\n",
      "|5577150313|2025-11-03|Lunch    |Apple     |402.43442|false          |78.0     |1.84    |6.4        |\n",
      "+----------+----------+---------+----------+---------+---------------+---------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "|   user_id|      date|total_Calories (kcal)|total_Protein (g)|total_Carbohydrates (g)|total_Fat (g)|total_Fiber (g)|total_Sugars (g)|total_Sodium (mg)|total_Cholesterol (mg)|total_Water_Intake (ml)|sleep_hours|target_steps|target_calories_burnt|\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "|5577150313|2025-11-01|               2406.0|        112.54942|              301.81903|     69.55344|      31.924044|        64.14415|        1833.1382|             249.48315|              2828.8123|   7.271114|      8412.0|               1933.0|\n",
      "|5577150313|2025-11-02|               2399.0|         112.1454|              287.58096|     71.47573|       29.85956|       57.979183|        1802.9264|             211.11894|              2876.1096|   8.005759|      7841.0|               2025.0|\n",
      "|5577150313|2025-11-03|               2357.0|       106.844055|              273.71988|     62.98691|      31.482908|       56.271103|        1858.1335|             212.49597|               2938.837|   8.008408|      6591.0|               2007.0|\n",
      "|5577150313|2025-11-04|               2354.0|          109.938|               307.4467|    70.825005|      31.519148|       60.012505|        1885.2391|             212.51869|              2921.4182|  7.7688737|      7808.0|               1986.0|\n",
      "|5577150313|2025-11-05|               2408.0|        108.92812|              309.42773|     64.70955|      31.530674|       55.888752|        1913.4246|              245.3716|               2942.518|  7.6842036|      7662.0|               1922.0|\n",
      "|5577150313|2025-11-06|               2397.0|        106.93392|              294.94672|     71.94473|      32.977352|        57.69667|        1840.2289|             222.71182|              2832.6624|  7.7644134|      7079.0|               1869.0|\n",
      "|5577150313|2025-11-07|               2428.0|        114.04051|               283.6027|     70.74784|       31.96666|       56.941204|        1898.6005|             229.73735|              2944.7446|  7.5697923|      7104.0|               1963.0|\n",
      "|5577150313|2025-11-08|               2335.0|         106.2436|               306.8073|     68.99558|      29.277191|       60.237373|         1956.208|             241.59966|              2774.4316|  7.5242624|      8440.0|               1968.0|\n",
      "|5577150313|2025-11-09|               2394.0|        109.77015|               285.5786|     64.06373|      30.597742|        56.27118|        1845.9186|             243.42638|               2730.312|  7.8401604|      8178.0|               1982.0|\n",
      "|5577150313|2025-11-10|               2453.0|        111.42334|              281.50717|     66.87288|      29.945042|       60.012028|        1948.6594|             241.87167|              2880.3313|  7.4728985|      7964.0|               1930.0|\n",
      "+----------+----------+---------------------+-----------------+-----------------------+-------------+---------------+----------------+-----------------+----------------------+-----------------------+-----------+------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, DateType\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "# --- Load your real meal dataset ---\n",
    "food_df = spark.read.csv(\"./food_nutrition_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Expected columns: e.g., [\"food_name\", \"meal_type\", \"calories\", \"protein\", ...]\n",
    "# If your column names differ, adjust below\n",
    "meal_types = [\"Breakfast\", \"Lunch\", \"Dinner\", \"Snack\"]\n",
    "\n",
    "# --- Define realistic water intake ranges (ml) per meal type ---\n",
    "water_ranges = {\n",
    "    \"Breakfast\": (250, 500),\n",
    "    \"Lunch\": (400, 700),\n",
    "    \"Dinner\": (400, 700),\n",
    "    \"Snack\": (150, 300)\n",
    "}\n",
    "\n",
    "# --- Parameters ---\n",
    "start_date = datetime.date(2025, 11, 1)\n",
    "end_date = datetime.date(2025, 11, 30)\n",
    "num_users = 5  # adjustable\n",
    "\n",
    "# --- Convert food_df to Python lists for random sampling ---\n",
    "food_by_type = {}\n",
    "for m in meal_types:\n",
    "    food_by_type[m] = (\n",
    "        food_df.filter(food_df[\"meal_type\"] == m)\n",
    "        .select(\"food_item\")\n",
    "        .rdd.flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "# --- Generate dummy data ---\n",
    "rows = []\n",
    "users_ids = [5577150313,5553957443,4020332650]\n",
    "for user_idx in users_ids:\n",
    "    user_id = f\"{user_idx}\"\n",
    "    height = round(random.uniform(1.65, 1.85), 2)\n",
    "    base_weight = round(random.uniform(60, 85), 1)\n",
    "\n",
    "    for day_offset in range((end_date - start_date).days + 1):\n",
    "        day = start_date + datetime.timedelta(days=day_offset)\n",
    "        sport_available = bool(random.choice([True, False]))\n",
    "        weight_today = round(base_weight + random.uniform(-1, 1), 1)\n",
    "        sleep_hours = round(random.uniform(4.5, 9.0), 1)\n",
    "\n",
    "        for meal_type in meal_types:\n",
    "            foods = food_by_type.get(meal_type, [])\n",
    "            food_choice = random.choice(foods) if foods else f\"Random {meal_type}\"\n",
    "            water_ml = random.uniform(*water_ranges[meal_type])\n",
    "            rows.append((user_id, day, meal_type, food_choice, water_ml,\n",
    "                         sport_available, weight_today, height, sleep_hours))\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"meal_type\", StringType(), True),\n",
    "    StructField(\"food_name\", StringType(), True),\n",
    "    StructField(\"water_ml\", FloatType(), True),\n",
    "    StructField(\"sport_available\", BooleanType(), True),\n",
    "    StructField(\"weight_kg\", FloatType(), True),\n",
    "    StructField(\"height_m\", FloatType(), True),\n",
    "    StructField(\"sleep_hours\", FloatType(), True)\n",
    "])\n",
    "plan_rows = []\n",
    "for user_idx in users_ids:\n",
    "    user_id = f\"{user_idx}\"\n",
    "\n",
    "    # base daily targets (adjustable per user)\n",
    "    base_targets = {\n",
    "        \"Calories\": float(random.randint(2000, 2700)),\n",
    "        \"Protein\": float(random.randint(90, 130)),\n",
    "        \"Carbohydrates\": float(random.randint(250, 400)),\n",
    "        \"Fat\": float(random.randint(60, 90)),\n",
    "        \"Fiber\": float(random.randint(25, 35)),\n",
    "        \"Sugars\": float(random.randint(40, 70)),\n",
    "        \"Sodium\": float(random.randint(1500, 2500)),\n",
    "        \"Cholesterol\": float(random.randint(200, 300)),\n",
    "        \"Water_Intake\": float(random.randint(2000, 3000)),\n",
    "        \"Sleep\": float(random.uniform(6.5, 8.0)),\n",
    "        \"Steps\": float(random.randint(7000, 12000)),\n",
    "        \"Calories_Burnt\": float(random.randint(1800, 2500))\n",
    "    }\n",
    "\n",
    "    for day_offset in range((end_date - start_date).days + 1):\n",
    "        day = start_date + datetime.timedelta(days=day_offset)\n",
    "\n",
    "        # small day-to-day variation\n",
    "        row = (\n",
    "            user_id,\n",
    "            day,\n",
    "           float(base_targets[\"Calories\"] + random.randint(-100, 100)),\n",
    "            float(base_targets[\"Protein\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Carbohydrates\"] + random.uniform(-20, 20)),\n",
    "            float(base_targets[\"Fat\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Fiber\"] + random.uniform(-2, 2)),\n",
    "            float(base_targets[\"Sugars\"] + random.uniform(-5, 5)),\n",
    "            float(base_targets[\"Sodium\"] + random.uniform(-100, 100)),\n",
    "            float(base_targets[\"Cholesterol\"] + random.uniform(-20, 20)),\n",
    "            float(base_targets[\"Water_Intake\"] + random.uniform(-150, 150)),\n",
    "            float(base_targets[\"Sleep\"] + random.uniform(-0.5, 0.5)),\n",
    "            float(base_targets[\"Steps\"] + random.randint(-1000, 1000)),\n",
    "            float(base_targets[\"Calories_Burnt\"] + random.randint(-150, 150))\n",
    "        )\n",
    "\n",
    "        plan_rows.append(row)\n",
    "\n",
    "schema_plan = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"total_Calories (kcal)\", FloatType(), True),\n",
    "    StructField(\"total_Protein (g)\", FloatType(), True),\n",
    "    StructField(\"total_Carbohydrates (g)\", FloatType(), True),\n",
    "    StructField(\"total_Fat (g)\", FloatType(), True),\n",
    "    StructField(\"total_Fiber (g)\", FloatType(), True),\n",
    "    StructField(\"total_Sugars (g)\", FloatType(), True),\n",
    "    StructField(\"total_Sodium (mg)\", FloatType(), True),\n",
    "    StructField(\"total_Cholesterol (mg)\", FloatType(), True),\n",
    "    StructField(\"total_Water_Intake (ml)\", FloatType(), True),\n",
    "    StructField(\"sleep_hours\", FloatType(), True),\n",
    "    StructField(\"target_steps\", FloatType(), True),\n",
    "    StructField(\"target_calories_burnt\", FloatType(), True),\n",
    "])\n",
    "\n",
    "df_monthly_plan = spark.createDataFrame(plan_rows, schema_plan)\n",
    "# --- Create DataFrame ---\n",
    "df_daily_meals = spark.createDataFrame(rows, schema)\n",
    "\n",
    "# --- Show sample ---\n",
    "df_daily_meals.show(10, truncate=False)\n",
    "df_monthly_plan.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e81f2f8-4889-42a2-bcc7-b99d4e3f895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "from google.cloud import firestore\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, DateType\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Logging Setup\n",
    "# ----------------------------\n",
    "fs_logger = logging.getLogger(\"firestore-writer\")\n",
    "fs_logger.setLevel(logging.INFO)\n",
    "if not fs_logger.hasHandlers():\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    fs_logger.addHandler(h)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Helper Functions\n",
    "# ----------------------------\n",
    "def _serialize_value(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (datetime.datetime, datetime.date)):\n",
    "        return v.isoformat()\n",
    "    if isinstance(v, decimal.Decimal):\n",
    "        return float(v)\n",
    "    if isinstance(v, (bytes, bytearray)):\n",
    "        try:\n",
    "            return v.decode()\n",
    "        except Exception:\n",
    "            return str(v)\n",
    "    try:\n",
    "        json.dumps(v)\n",
    "        return v\n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "\n",
    "def _commit_with_retries(batch_obj, max_retries=3, base_backoff=0.5):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch_obj.commit()\n",
    "            return\n",
    "        except (GoogleAPICallError, RetryError, IOError) as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                fs_logger.exception(\"Firestore commit failed after %d attempts\", attempt - 1)\n",
    "                raise\n",
    "            backoff = base_backoff * (2 ** (attempt - 1))\n",
    "            fs_logger.warning(\n",
    "                \"Transient error committing firestore batch (attempt %d). Backing off %.2fs. Error: %s\",\n",
    "                attempt, backoff, str(e)\n",
    "            )\n",
    "            time.sleep(backoff)\n",
    "\n",
    "\n",
    "def make_firestore_writer(collection_name, firestore_client, batch_size=500, max_retries=3):\n",
    "    def write_batch_to_firestore(batch_df, epoch_id=None):\n",
    "        rows = batch_df.count()\n",
    "        if not rows:\n",
    "            fs_logger.info(\"[epoch %s] empty, skipping collection=%s\", str(epoch_id), collection_name)\n",
    "            return\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] writing %s rows to Firestore collection '%s'\",\n",
    "                       str(epoch_id), rows, collection_name)\n",
    "\n",
    "        docs_written = 0\n",
    "        ops_in_current_batch = 0\n",
    "        fs_batch = firestore_client.batch()\n",
    "\n",
    "        for row in batch_df.toLocalIterator():\n",
    "            data = row.asDict(recursive=True)\n",
    "            user_id = data.get(\"user_id\")\n",
    "\n",
    "            if not user_id:\n",
    "                fs_logger.warning(\"[epoch %s] skipping row without user_id\", str(epoch_id))\n",
    "                continue\n",
    "\n",
    "            for k, v in list(data.items()):\n",
    "                data[k] = _serialize_value(v)\n",
    "\n",
    "            doc_ref = (\n",
    "                firestore_client.collection(\"users\")\n",
    "                .document(str(user_id))\n",
    "                .collection(collection_name)\n",
    "                .document()\n",
    "            )\n",
    "\n",
    "            fs_batch.set(doc_ref, data)\n",
    "            ops_in_current_batch += 1\n",
    "\n",
    "            if ops_in_current_batch >= batch_size:\n",
    "                _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "                docs_written += ops_in_current_batch\n",
    "                ops_in_current_batch = 0\n",
    "                fs_batch = firestore_client.batch()\n",
    "\n",
    "        if ops_in_current_batch > 0:\n",
    "            _commit_with_retries(fs_batch, max_retries=max_retries)\n",
    "            docs_written += ops_in_current_batch\n",
    "\n",
    "        fs_logger.info(\"[epoch %s] wrote %d docs under users/*/%s/\",\n",
    "                       str(epoch_id), docs_written, collection_name)\n",
    "    return write_batch_to_firestore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27b61ce-7faf-49b2-ad65-94fb5e41fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-10 10:45:26,940 - [epoch 2025-11-07] writing 360 rows to Firestore collection 'daily_meals'\n",
      "[INFO] 2025-11-10 10:45:29,957 - [epoch 2025-11-07] wrote 360 docs under users/*/daily_meals/\n",
      "[INFO] 2025-11-10 10:45:30,177 - [epoch 2025-11-07] writing 90 rows to Firestore collection 'monthly_plan'\n",
      "[INFO] 2025-11-10 10:45:31,848 - [epoch 2025-11-07] wrote 90 docs under users/*/monthly_plan/\n",
      "[INFO] 2025-11-10 10:45:31,849 - âœ… Finished writing dummy data to Firestore.\n"
     ]
    }
   ],
   "source": [
    "writer = make_firestore_writer(\"daily_meals\", db)\n",
    "\n",
    "# Run the batch write\n",
    "writer(df_daily_meals, epoch_id=\"2025-11-07\")\n",
    "writer = make_firestore_writer(\"monthly_plan\", db)\n",
    "writer(df_monthly_plan, epoch_id=\"2025-11-07\")\n",
    "\n",
    "fs_logger.info(\"âœ… Finished writing dummy data to Firestore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49002d26-6b63-41ce-83b6-4f64c7f1f545",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## import and calculate daily metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd33f8-9fbf-4089-8c52-5a32f4121170",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a1fd3bb-b977-4c34-90e4-c2b941d00b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    avg,\n",
    "    expr,\n",
    "    to_timestamp,when, sum as _sum,\n",
    ")\n",
    "\n",
    "\n",
    "def get_user_ids(db, collection_name=\"users\", debug=False):\n",
    "    \"\"\"\n",
    "    Robustly enumerate top-level user document IDs.\n",
    "    Tries, in order:\n",
    "      1) collection_ref.list_documents()\n",
    "      2) collection_ref.get()\n",
    "      3) collection_group (search for nested 'users' if top-level empty)\n",
    "    Returns list of user_ids (strings).\n",
    "    \"\"\"\n",
    "    user_ids = []\n",
    "\n",
    "\n",
    "    try:\n",
    "        coll_ref = db.collection(collection_name)\n",
    "\n",
    "        # 1) Try list_documents() (lightweight, returns DocumentReference objects)\n",
    "        try:\n",
    "            docs = list(coll_ref.list_documents())\n",
    "            if docs:\n",
    "                user_ids = [d.id for d in docs]\n",
    "                if debug:\n",
    "                    print(f\"âœ… get_user_ids: using list_documents(), found {len(user_ids)} users.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"â„¹ï¸ list_documents() returned 0 refs; falling back to get().\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"âš ï¸ list_documents() failed:\", repr(e))\n",
    "\n",
    "        # 2) Fallback: collection.get() (reads documents)\n",
    "        try:\n",
    "            docs = list(coll_ref.get())\n",
    "            if docs:\n",
    "                user_ids = [d.id for d in docs]\n",
    "                if debug:\n",
    "                    print(f\"âœ… get_user_ids: using get(), found {len(user_ids)} users.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"â„¹ï¸ collection.get() returned 0 documents; falling back to collection_group().\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"âš ï¸ collection.get() failed:\", repr(e))\n",
    "\n",
    "        # 3) Last resort: collection_group search (if 'users' lives nested under other paths)\n",
    "        try:\n",
    "            group_docs = list(db.collection_group(collection_name).limit(500).stream())\n",
    "            if group_docs:\n",
    "                # Use parent document id if the structure is like parent/{parent_id}/users/{user_id}\n",
    "                user_ids = [d.reference.id for d in group_docs]\n",
    "                if debug:\n",
    "                    print(f\"âœ… get_user_ids: using collection_group('{collection_name}'), found {len(user_ids)} documents.\")\n",
    "                return user_ids\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"â„¹ï¸ collection_group('{collection_name}') returned 0 results.\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(\"âš ï¸ collection_group() failed:\", repr(e))\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(\"âŒ get_user_ids: unexpected error:\", repr(e))\n",
    "\n",
    "    # final: empty result\n",
    "    if debug:\n",
    "        print(\"âŒ get_user_ids: no users found by any method.\")\n",
    "    return []\n",
    "\n",
    "# -------------------------\n",
    "# Example: read_all_users using get_user_ids\n",
    "# -------------------------\n",
    "def read_all_users(db, collection_name=\"users\", batch_size=500, debug=False):\n",
    "    \"\"\"\n",
    "    Read user subcollections per user id using robust enumeration.\n",
    "    Returns list of records (dicts) same as earlier fetch_user_data would produce.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_ids = get_user_ids(db, collection_name=collection_name, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"ðŸ“‹ Found {len(user_ids)} user doc ids.\")\n",
    "\n",
    "        all_records = []\n",
    "        for user_id in user_ids:\n",
    "            if debug:\n",
    "                print(f\"Processing user: {user_id}\")\n",
    "            try:\n",
    "                # Reuse your existing fetch_user_data(db, user_id, ...) here.\n",
    "                # Example minimal call (replace with your function):\n",
    "                user_records = fetch_user_data(db, user_id, page_size=batch_size)  # assume function exists\n",
    "                if not user_records and debug:\n",
    "                    print(f\"â„¹ï¸ No subcollection docs for user {user_id}.\")\n",
    "                all_records.extend(user_records)\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"âš ï¸ Failed reading data for user {user_id}: {e}\")\n",
    "        return all_records\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(\"âŒ read_all_users error:\", repr(e))\n",
    "        return []# --- Pagination helper ---\n",
    "def paginate_collection(collection_ref, page_size=500):\n",
    "    try:\n",
    "        docs = collection_ref.limit(page_size).stream()\n",
    "        last_doc = None\n",
    "        while True:\n",
    "            batch = list(docs)\n",
    "            if not batch:\n",
    "                break\n",
    "            yield batch\n",
    "            last_doc = batch[-1]\n",
    "            docs = collection_ref.start_after(last_doc).limit(page_size).stream()\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Pagination failed:\", e)\n",
    "\n",
    "# --- Fetch a single user's subcollections with debugging ---\n",
    "def fetch_user_data(db, user_id, page_size=500, debug=False):\n",
    "    user_ref = db.collection(\"users\").document(user_id)\n",
    "    records = []\n",
    "    for metric in [\"calories\", \"heart_rate\", \"steps\"]:\n",
    "        try:\n",
    "            sub_ref = user_ref.collection(metric)\n",
    "            all_docs = []\n",
    "            for batch in paginate_collection(sub_ref, page_size):\n",
    "                for doc in batch:\n",
    "                    all_docs.append(doc)\n",
    "            if debug:\n",
    "                print(f\"ðŸ§© User {user_id} | {metric}: {len(all_docs)} docs\")\n",
    "\n",
    "            for doc in all_docs:\n",
    "                data = doc.to_dict() or {}\n",
    "                if debug and data:\n",
    "                    print(f\"   â†³ sample doc: {data}\")\n",
    "\n",
    "                # detect possible timestamp and value fields\n",
    "                timestamp_field = None\n",
    "                for k in [\"timestamp\", \"time\", \"created_at\", \"date\"]:\n",
    "                    if k in data:\n",
    "                        timestamp_field = k\n",
    "                        break\n",
    "\n",
    "                value_field = None\n",
    "                for k in [\"value\", \"heart_rate\", \"calories\", \"steps\", \"count\"]:\n",
    "                    if k in data:\n",
    "                        value_field = k\n",
    "                        break\n",
    "\n",
    "                if not timestamp_field or not value_field:\n",
    "                    if debug:\n",
    "                        print(f\"   âš ï¸ Missing timestamp/value in {metric} doc: {data}\")\n",
    "                    continue\n",
    "\n",
    "                rec = {\n",
    "                    \"user_id\": user_id,\n",
    "                    \"metric_type\": metric,\n",
    "                    \"timestamp\": data[timestamp_field],\n",
    "                    \"value\": float(data[value_field]) if data[value_field] is not None else 0.0,\n",
    "                }\n",
    "                records.append(rec)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to read {metric} for user {user_id}: {e}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "\n",
    "# --- Build DataFrame safely ---\n",
    "def build_dataframe(spark, raw_data):\n",
    "    schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"metric_type\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "    ])\n",
    "    try:\n",
    "        df = spark.createDataFrame(raw_data or [], schema=schema)\n",
    "        print(f\"âœ… DataFrame created with {df.count()} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error creating DataFrame:\", e)\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    # --- Safely cast string timestamp -> proper timestamp ---\n",
    "    try:\n",
    "        df = df.withColumn(\n",
    "            \"timestamp\",\n",
    "            to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Failed to cast timestamp column:\", repr(e))\n",
    "# --- Core logic ---\n",
    "def get_last24h_and_averages(spark, db, batch_size=300, debug=False):\n",
    "    raw_data = read_all_users(db, batch_size=batch_size, debug=debug)\n",
    "    print(f\"âœ… Total records fetched: {len(raw_data)}\")\n",
    "\n",
    "    df = build_dataframe(spark, raw_data)\n",
    "    if df.isEmpty():\n",
    "        print(\"â„¹ï¸ No data available.\")\n",
    "        return df, df\n",
    "\n",
    "    df_recent = df.filter(col(\"timestamp\") >= expr(\"current_timestamp() - INTERVAL 24 HOURS\"))\n",
    "    print(f\"ðŸ•’ Records after 24h filter: {df_recent.count()}\")\n",
    "\n",
    "    df_avg = (\n",
    "        df_recent.groupBy(\"user_id\", \"metric_type\")\n",
    "        .agg(\n",
    "            # Conditional aggregation: use avg for heart_rate, sum for others\n",
    "            avg(when(col(\"metric_type\") == \"heart_rate\", col(\"value\"))).alias(\"heartbeat_avg\"),\n",
    "            _sum(when(col(\"metric_type\") != \"heart_rate\", col(\"value\"))).alias(\"others_sum\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"daily_value\",\n",
    "            when(col(\"metric_type\") == \"heart_rate\", col(\"heartbeat_avg\"))\n",
    "            .otherwise(col(\"others_sum\"))\n",
    "        )\n",
    "        .select(\"user_id\", \"metric_type\", \"daily_value\")\n",
    "        .orderBy(\"user_id\", \"metric_type\")\n",
    "    )\n",
    "\n",
    "    return df_recent, df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac7532-85ff-42e9-9d82-8d16f6245e8b",
   "metadata": {},
   "source": [
    "### excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92bf7329-58b3-4c21-a794-2c0e8ddcef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… get_user_ids: using list_documents(), found 33 users.\n",
      "ðŸ“‹ Found 33 user doc ids.\n",
      "Processing user: 1503960366\n",
      "Processing user: 1644430081\n",
      "Processing user: 1844505072\n",
      "Processing user: 1927972279\n",
      "Processing user: 2022484408\n",
      "Processing user: 2026352035\n",
      "Processing user: 2320127002\n",
      "Processing user: 2347167796\n",
      "Processing user: 2873212765\n",
      "Processing user: 3372868164\n",
      "Processing user: 3977333714\n",
      "Processing user: 4020332650\n",
      "Processing user: 4057192912\n",
      "Processing user: 4319703577\n",
      "Processing user: 4388161847\n",
      "Processing user: 4445114986\n",
      "Processing user: 4558609924\n",
      "Processing user: 4702921684\n",
      "Processing user: 5553957443\n",
      "Processing user: 5577150313\n",
      "Processing user: 6290855005\n",
      "Processing user: 6775888955\n",
      "Processing user: 6962181067\n",
      "Processing user: 7007744171\n",
      "Processing user: 7086361926\n",
      "Processing user: 8053475328\n",
      "Processing user: 8378563200\n",
      "Processing user: 8583815059\n",
      "Processing user: 8792009665\n",
      "Processing user: 8877689391\n",
      "Processing user: user_4020332650\n",
      "â„¹ï¸ No subcollection docs for user user_4020332650.\n",
      "Processing user: user_5553957443\n",
      "â„¹ï¸ No subcollection docs for user user_5553957443.\n",
      "Processing user: user_5577150313\n",
      "â„¹ï¸ No subcollection docs for user user_5577150313.\n",
      "âœ… Total records fetched: 10471\n",
      "âœ… DataFrame created with 10471 rows.\n",
      "ðŸ•’ Records after 24h filter: 260\n"
     ]
    }
   ],
   "source": [
    "df_recent, df_daily_activity = get_last24h_and_averages(spark, db, batch_size=200, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5112f117-5038-4202-9791-39511e52dca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-07 14:56:31,979 - Pivoting long-format activity DF (metric_type/daily_value).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVITY PIVOTED COLUMNS: ['user_id', 'total_calories_burnt', 'avg_heart_rate', 'total_steps']\n"
     ]
    }
   ],
   "source": [
    "print(\"ACTIVITY PIVOTED COLUMNS:\", _pivot_activity_if_needed(df_daily_activity).columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae0e4e-c616-4908-a8ff-5520714261da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## daily planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dd9de-a76d-41b6-a67f-fdcc177c6a5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### join helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c580e2d8-eb77-4003-8a0d-5edb17c046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"fatigue-detector\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\"))\n",
    "    logger.addHandler(h)\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "\n",
    "def _find_user_id_col(df: DataFrame):\n",
    "    \"\"\"Return the name of the user-id-like column, or None.\"\"\"\n",
    "    candidates = [\"user_id\", \"userid\", \"UserID\", \"User_Id\", \"uid\", \"user\"]\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _normalize_user_id_column(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Ensure there is a 'user_id' column that's trimmed and lowercased.\"\"\"\n",
    "    col_name = _find_user_id_col(df)\n",
    "    if col_name is None:\n",
    "        # If no column looks like a user id, return as-is (will fail later)\n",
    "        logger.warning(\"No user-id-like column found in DataFrame. Columns: %s\", df.columns)\n",
    "        return df\n",
    "    # create/replace normalized 'user_id' string column\n",
    "    return df.withColumn(\"user_id\", F.lower(F.trim(F.col(col_name))))\n",
    "\n",
    "def _pivot_activity_if_needed(activity_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Pivot long-format activity df into wide; leave as-is if already wide.\"\"\"\n",
    "    cols = set(activity_df.columns)\n",
    "    if \"avg_heart_rate\" in cols or \"total_steps\" in cols:\n",
    "        logger.info(\"Activity DF already wide-format.\")\n",
    "        # still normalize user_id\n",
    "        return activity_df\n",
    "    if \"metric_type\" in cols and \"daily_value\" in cols:\n",
    "        logger.info(\"Pivoting long-format activity DF (metric_type/daily_value).\")\n",
    "        pivoted = (\n",
    "            activity_df\n",
    "            .groupBy(\"user_id\")\n",
    "            .pivot(\"metric_type\")\n",
    "            .agg(F.first(\"daily_value\"))\n",
    "        )\n",
    "        # rename common metrics if present\n",
    "        rename_map = {}\n",
    "        if \"heart_rate\" in pivoted.columns:\n",
    "            rename_map[\"heart_rate\"] = \"avg_heart_rate\"\n",
    "        if \"steps\" in pivoted.columns:\n",
    "            rename_map[\"steps\"] = \"total_steps\"\n",
    "        if \"calories\" in pivoted.columns:\n",
    "            rename_map[\"calories\"] = \"total_calories_burnt\"\n",
    "        out = pivoted\n",
    "        for old, new in rename_map.items():\n",
    "            out = out.withColumnRenamed(old, new)\n",
    "        return out\n",
    "    logger.warning(\"Activity DF does not have expected columns (avg_heart_rate/metric_type). Columns: %s\", activity_df.columns)\n",
    "    return activity_df\n",
    "\n",
    "def _safe_rename_nutrition_cols(nutrition_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Rename nutrition columns with spaces/units into safe snake_case names.\"\"\"\n",
    "    rename_map = {\n",
    "        \"Calories (kcal)\": \"calories_kcal\",\n",
    "        \"Protein (g)\": \"protein_g\",\n",
    "        \"Carbohydrates (g)\": \"carbs_g\",\n",
    "        \"Fat (g)\": \"fat_g\",\n",
    "        \"Fiber (g)\": \"fiber_g\",\n",
    "        \"Sugars (g)\": \"sugars_g\",\n",
    "        \"Sodium (mg)\": \"sodium_mg\",\n",
    "        \"Cholesterol (mg)\": \"cholesterol_mg\",\n",
    "        \"Water_Intake (ml)\": \"water_ml\"\n",
    "    }\n",
    "    df = nutrition_df\n",
    "    for old, new in rename_map.items():\n",
    "        if old in df.columns:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "        # also handle total_ prefixed\n",
    "        total_old = f\"total_{old}\"\n",
    "        total_new = f\"total_{new}\"\n",
    "        if total_old in df.columns:\n",
    "            df = df.withColumnRenamed(total_old, total_new)\n",
    "    return df\n",
    "\n",
    "def _detect_calorie_and_water_cols(nutrition_df: DataFrame):\n",
    "    calories_candidates = [\"total_calories_kcal\", \"calories_kcal\", \"total_Calories (kcal)\", \"Calories (kcal)\"]\n",
    "    water_candidates = [\"total_water_ml\", \"water_ml\", \"total_Water_Intake (ml)\", \"Water_Intake (ml)\"]\n",
    "    calories_col = next((c for c in calories_candidates if c in nutrition_df.columns), None)\n",
    "    water_col = next((c for c in water_candidates if c in nutrition_df.columns), None)\n",
    "    return calories_col, water_col\n",
    "\n",
    "# ------------------ Core pipeline ------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664e74a-248f-4f42-83f5-a72ddee9ec43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### compute fatigue functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce503c7-aeee-4237-93f0-9b8326cacd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-10 10:48:13,863 - Activity columns: ['user_id', 'metric_type', 'daily_value']\n",
      "[INFO] 2025-11-10 10:48:13,865 - Nutrition columns: ['user_id', 'date', 'meal_type', 'food_name', 'water_ml', 'sport_available', 'weight_kg', 'height_m', 'sleep_hours']\n",
      "[INFO] 2025-11-10 10:48:14,485 - Counts: activity=12 rows, nutrition=360 rows\n",
      "[INFO] 2025-11-10 10:48:15,162 - Sample user_ids activity: ['5553957443', '1503960366', '5577150313', '8378563200', '1644430081', '6962181067', '1844505072', '4020332650']\n",
      "[INFO] 2025-11-10 10:48:15,163 - Sample user_ids nutrition: ['5577150313', '5553957443', '4020332650']\n",
      "[INFO] 2025-11-10 10:48:15,163 - Pivoting long-format activity DF (metric_type/daily_value).\n",
      "[INFO] 2025-11-10 10:48:15,791 - Detected nutrition columns -> calories: None, water: water_ml\n",
      "[INFO] 2025-11-10 10:48:16,783 - Inner join resulted in 360 rows\n"
     ]
    }
   ],
   "source": [
    "def detect_daily_fatigue_with_diagnostics(raw_activity_df: DataFrame, raw_nutrition_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detect fatigue per user, with diagnostics when join produces empty results.\n",
    "    Returns DataFrame of fatigue summary. If join fails, returns diagnostic DataFrame (one row per dataset).\n",
    "    \"\"\"\n",
    "    # Normalize user id columns\n",
    "    activity_df = _normalize_user_id_column(raw_activity_df)\n",
    "    nutrition_df = _normalize_user_id_column(raw_nutrition_df)\n",
    "\n",
    "    logger.info(\"Activity columns: %s\", activity_df.columns)\n",
    "    logger.info(\"Nutrition columns: %s\", nutrition_df.columns)\n",
    "\n",
    "    # Show counts (cheap)\n",
    "    try:\n",
    "        a_count = activity_df.count()\n",
    "        n_count = nutrition_df.count()\n",
    "        logger.info(\"Counts: activity=%d rows, nutrition=%d rows\", a_count, n_count)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not count frames: %s\", e)\n",
    "\n",
    "    # Show sample user_ids\n",
    "    try:\n",
    "        a_sample = [r[\"user_id\"] for r in activity_df.select(\"user_id\").distinct().limit(20).collect()] if \"user_id\" in activity_df.columns else []\n",
    "        n_sample = [r[\"user_id\"] for r in nutrition_df.select(\"user_id\").distinct().limit(20).collect()] if \"user_id\" in nutrition_df.columns else []\n",
    "        logger.info(\"Sample user_ids activity: %s\", a_sample)\n",
    "        logger.info(\"Sample user_ids nutrition: %s\", n_sample)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not sample user_ids: %s\", e)\n",
    "\n",
    "    # Pivot activity if needed (and normalize its user_id again after pivot)\n",
    "    activity_pivoted = _pivot_activity_if_needed(activity_df).withColumn(\"user_id\", F.lower(F.trim(F.col(\"user_id\"))))\n",
    "\n",
    "    # Rename nutrition columns to safe names\n",
    "    nutrition_safe = _safe_rename_nutrition_cols(nutrition_df)\n",
    "\n",
    "    # detect nutrition calories/water column names\n",
    "    calories_col, water_col = _detect_calorie_and_water_cols(nutrition_safe)\n",
    "    logger.info(\"Detected nutrition columns -> calories: %s, water: %s\", calories_col, water_col)\n",
    "\n",
    "    # pick date expression\n",
    "    if \"date\" in activity_pivoted.columns and \"date\" in nutrition_safe.columns:\n",
    "        date_expr = F.coalesce(F.col(\"a.date\"), F.col(\"n.date\")).alias(\"date\")\n",
    "    elif \"date\" in activity_pivoted.columns:\n",
    "        date_expr = F.col(\"a.date\").alias(\"date\")\n",
    "    elif \"date\" in nutrition_safe.columns:\n",
    "        date_expr = F.col(\"n.date\").alias(\"date\")\n",
    "    else:\n",
    "        date_expr = F.current_date().alias(\"date\")\n",
    "\n",
    "    # Build joined DF (use pivoted activity)\n",
    "    sel = [\n",
    "        F.col(\"a.user_id\").alias(\"user_id\"),\n",
    "        date_expr,\n",
    "        F.col(\"a.avg_heart_rate\").alias(\"avg_heart_rate\"),\n",
    "        F.col(\"a.total_steps\").alias(\"total_steps\"),\n",
    "        F.col(\"a.total_calories_burnt\").alias(\"total_calories_burnt\"),\n",
    "        (F.col(f\"n.{calories_col}\") if calories_col is not None else F.lit(None)).alias(\"calories_intake\"),\n",
    "        F.col(\"n.sleep_hours\").alias(\"sleep_hours\"),\n",
    "        (F.col(\"n.weight\") if \"weight\" in nutrition_safe.columns else (F.col(\"n.weight_kg\") if \"weight_kg\" in nutrition_safe.columns else F.lit(None))).alias(\"weight\"),\n",
    "        (F.col(\"n.height\") if \"height\" in nutrition_safe.columns else (F.col(\"n.height_m\") if \"height_m\" in nutrition_safe.columns else F.lit(None))).alias(\"height\"),\n",
    "        (F.col(f\"n.{water_col}\") if water_col is not None else F.lit(None)).alias(\"water_ml\"),\n",
    "    ]\n",
    "\n",
    "    joined_inner = (\n",
    "        activity_pivoted.alias(\"a\")\n",
    "        .join(nutrition_safe.alias(\"n\"), on=\"user_id\", how=\"inner\")\n",
    "        .select(*sel)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        joined_count = joined_inner.count()\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not count joined_inner: %s\", e)\n",
    "        joined_count = 0\n",
    "\n",
    "    logger.info(\"Inner join resulted in %d rows\", joined_count)\n",
    "\n",
    "    if joined_count == 0:\n",
    "        # Diagnose mismatch: find set difference of user ids if possible\n",
    "        logger.warning(\"Inner join is empty â€” diagnosing user id mismatches.\")\n",
    "        a_uids = set([r[\"user_id\"] for r in activity_pivoted.select(\"user_id\").distinct().collect()]) if \"user_id\" in activity_pivoted.columns else set()\n",
    "        n_uids = set([r[\"user_id\"] for r in nutrition_safe.select(\"user_id\").distinct().collect()]) if \"user_id\" in nutrition_safe.columns else set()\n",
    "\n",
    "        logger.info(\"Unique user_ids in activity (sample 50): %s\", list(a_uids)[:50])\n",
    "        logger.info(\"Unique user_ids in nutrition (sample 50): %s\", list(n_uids)[:50])\n",
    "\n",
    "        only_in_activity = list(a_uids - n_uids)[:20]\n",
    "        only_in_nutrition = list(n_uids - a_uids)[:20]\n",
    "        logger.info(\"User_ids only in activity (sample): %s\", only_in_activity)\n",
    "        logger.info(\"User_ids only in nutrition (sample): %s\", only_in_nutrition)\n",
    "\n",
    "        # Fallback: perform left join to keep activity rows and attach nutrition where present\n",
    "        joined_left = (\n",
    "            activity_pivoted.alias(\"a\")\n",
    "            .join(nutrition_safe.alias(\"n\"), on=\"user_id\", how=\"left\")\n",
    "            .select(*sel)\n",
    "        )\n",
    "\n",
    "        logger.info(\"Left join result count: %d\", joined_left.count())\n",
    "        # compute factors on left-joined frame (so we at least return activity-driven fatigue)\n",
    "        result = _compute_physiologic_factors(joined_left)\n",
    "        # add a diagnostics column\n",
    "        result = result.withColumn(\"join_diagnostic\", F.lit(\"inner_empty_fell_back_to_left\"))\n",
    "        return result\n",
    "\n",
    "    # else inner join has data -> compute factors\n",
    "    result = _compute_physiologic_factors(joined_inner)\n",
    "    result = result.withColumn(\"join_diagnostic\", F.lit(\"inner_ok\"))\n",
    "    return result\n",
    "\n",
    "# Reuse earlier _compute_physiologic_factors implementation (copy from your working version)\n",
    "def _compute_physiologic_factors(df: DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"total_steps\", F.coalesce(F.col(\"total_steps\"), F.lit(0)))\n",
    "    df = df.withColumn(\"avg_heart_rate\", F.coalesce(F.col(\"avg_heart_rate\"), F.lit(70)))\n",
    "    df = df.withColumn(\"total_calories_burnt\", F.coalesce(F.col(\"total_calories_burnt\"), F.lit(2000)))\n",
    "    df = df.withColumn(\"calories_intake\", F.coalesce(F.col(\"calories_intake\"), F.lit(0)))\n",
    "    df = df.withColumn(\"water_ml\", F.coalesce(F.col(\"water_ml\"), F.lit(0)))\n",
    "    df = df.withColumn(\"sleep_hours\", F.coalesce(F.col(\"sleep_hours\"), F.lit(7.0)))\n",
    "    df = df.withColumn(\"weight\", F.coalesce(F.col(\"weight\"), F.lit(70.0)))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"sleep_factor\",\n",
    "        F.when(F.col(\"sleep_hours\") < 5, 0.3)\n",
    "         .when(F.col(\"sleep_hours\") < 6, 0.5)\n",
    "         .when(F.col(\"sleep_hours\") < 7, 0.8)\n",
    "         .when(F.col(\"sleep_hours\") <= 8, 1.0)\n",
    "         .otherwise(0.9)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"activity_factor\",\n",
    "        F.when(F.col(\"total_steps\") < 3000, 0.4)\n",
    "         .when(F.col(\"total_steps\") < 6000, 0.7)\n",
    "         .when(F.col(\"total_steps\") <= 10000, 1.0)\n",
    "         .when(F.col(\"total_steps\") <= 12000, 0.8)\n",
    "         .otherwise(0.6)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"heart_factor\",\n",
    "        F.when(F.col(\"avg_heart_rate\") <= 60, 1.0)\n",
    "         .when(F.col(\"avg_heart_rate\") <= 75, 0.8)\n",
    "         .when(F.col(\"avg_heart_rate\") <= 85, 0.6)\n",
    "         .otherwise(0.4)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"energy_ratio\",\n",
    "        F.when(F.col(\"total_calories_burnt\") > 0,\n",
    "               F.col(\"calories_intake\") / F.col(\"total_calories_burnt\"))\n",
    "         .otherwise(F.lit(0.0))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"energy_factor\",\n",
    "        F.when(F.col(\"energy_ratio\") < 0.7, 0.5)\n",
    "         .when(F.col(\"energy_ratio\") < 0.9, 0.8)\n",
    "         .when(F.col(\"energy_ratio\") <= 1.1, 1.0)\n",
    "         .otherwise(0.9)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"hydration_factor\",\n",
    "        F.least(F.col(\"water_ml\") / (F.col(\"weight\") * F.lit(35.0)), F.lit(1.0))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"fatigue_score\",\n",
    "        0.35 * F.col(\"sleep_factor\") +\n",
    "        0.20 * F.col(\"activity_factor\") +\n",
    "        0.15 * F.col(\"heart_factor\") +\n",
    "        0.20 * F.col(\"energy_factor\") +\n",
    "        0.10 * F.col(\"hydration_factor\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"fatigue_level\",\n",
    "        F.when(F.col(\"fatigue_score\") < 0.4, \"Exhausted\")\n",
    "         .when(F.col(\"fatigue_score\") < 0.65, \"Tired\")\n",
    "         .when(F.col(\"fatigue_score\") < 0.85, \"Normal\")\n",
    "         .otherwise(\"Energetic\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "result_df = detect_daily_fatigue_with_diagnostics(df_daily_activity, df_daily_meals)\n",
    "fatigue_df = result_df.drop(\"join_diagnostic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d63d3-6041-4449-9611-fc7af5919c7e",
   "metadata": {},
   "source": [
    "## generate daily plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab1c33-1178-488e-bd39-d9ce17d0d766",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### load the target plan of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d41d219-4bad-4c6c-ac3a-59571aca4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _to_date(raw):\n",
    "    if raw is None:\n",
    "        return None\n",
    "    if isinstance(raw, datetime.date) and not isinstance(raw, datetime.datetime):\n",
    "        return raw\n",
    "    if isinstance(raw, datetime.datetime):\n",
    "        return raw.date()\n",
    "    if isinstance(raw, str):\n",
    "        s = raw.strip()\n",
    "        try:\n",
    "            return datetime.date.fromisoformat(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return datetime.datetime.fromisoformat(s.replace(\"Z\", \"+00:00\")).date()\n",
    "            except Exception:\n",
    "                import re\n",
    "                m = re.search(r\"(\\d{4})[-/](\\d{2})[-/](\\d{2})\", s)\n",
    "                if m:\n",
    "                    y, mo, d = m.groups()\n",
    "                    try:\n",
    "                        return datetime.date(int(y), int(mo), int(d))\n",
    "                    except Exception:\n",
    "                        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_monthly_plan_for_date( db, target_date, debug=False):\n",
    "    \"\"\"\n",
    "    Load monthly_plan documents for all users for target_date and return a Spark DataFrame.\n",
    "    - spark: SparkSession\n",
    "    - db: firestore.Client()\n",
    "    - target_date: datetime.date | datetime.datetime | 'YYYY-MM-DD' string\n",
    "    \"\"\"\n",
    "    if isinstance(target_date, datetime.datetime):\n",
    "        target_date_str = target_date.date().isoformat()\n",
    "    elif isinstance(target_date, datetime.date):\n",
    "        target_date_str = target_date.isoformat()\n",
    "    else:\n",
    "        target_date_str = str(target_date).strip()\n",
    "\n",
    "    user_ids = get_user_ids(db)\n",
    "    if debug:\n",
    "        print(f\"Found {len(user_ids)} users\")\n",
    "    \n",
    "    rows = []\n",
    "    for uid in user_ids:\n",
    "\n",
    "        docs = db.collection(f\"users/{uid}/monthly_plan\").stream()\n",
    "        docs = [doc.to_dict() for doc in docs]\n",
    "        try:\n",
    "            for doc in docs:\n",
    "                doc[\"date\"] = target_date_str\n",
    "                rows.append(doc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Query failed for {uid}: {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        if debug:\n",
    "            print(f\"âš ï¸ No monthly_plan found for {target_date_str}\")\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"date\", DateType(), True),\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema=empty_schema)\n",
    "\n",
    "    # infer schema dynamically from first document\n",
    "    sample = rows[0]\n",
    "    fields = []\n",
    "    for k, v in sample.items():\n",
    "        if k == \"user_id\":\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "        elif k == \"date\":\n",
    "            fields.append(StructField(k, StringType(), True))  # keep as string for consistency\n",
    "        elif isinstance(v, (int, float)):\n",
    "            fields.append(StructField(k, DoubleType(), True))\n",
    "        else:\n",
    "            fields.append(StructField(k, StringType(), True))\n",
    "\n",
    "    schema = StructType(fields)\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d25463d-6574-48f1-ad29-b244792a2012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 users\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['total_Calories (kcal)',\n",
       " 'sleep_hours',\n",
       " 'total_Sugars (g)',\n",
       " 'total_Sodium (mg)',\n",
       " 'total_Fat (g)',\n",
       " 'target_steps',\n",
       " 'total_Cholesterol (mg)',\n",
       " 'date',\n",
       " 'target_calories_burnt',\n",
       " 'total_Carbohydrates (g)',\n",
       " 'user_id',\n",
       " 'total_Protein (g)',\n",
       " 'total_Fiber (g)',\n",
       " 'total_Water_Intake (ml)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plans = load_monthly_plan_for_date(db, \"2025-11-10\", debug=True)\n",
    "# create Spark DF if you want:\n",
    "df_plans.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b711f-0218-4fab-8615-f80f746cb6b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### load reference food and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703570a-39f8-486f-8914-44787b3540ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load datasets ---\n",
    "if not food_df:\n",
    "    food_df = spark.read.csv(\"./food_nutrition_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# --- 2. Define meal structure ---\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "\n",
    "# Broadcast static food data for performance\n",
    "food_broadcast = spark.sparkContext.broadcast(food_df.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "908d41f4-c1c1-430d-a8ca-128961fa91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------+---------+------------+---------+\n",
      "|Activity                   |Calories_Burned|Category |Duration_min|Intensity|\n",
      "+---------------------------+---------------+---------+------------+---------+\n",
      "|Rest                       |0              |Tired    |30          |Very Low |\n",
      "|Gentle Yoga                |80             |Tired    |30          |Low      |\n",
      "|Light Walk                 |100            |Recovery |20          |Low      |\n",
      "|Stretching Routine         |50             |Recovery |15          |Low      |\n",
      "|Brisk Walk                 |150            |Normal   |30          |Moderate |\n",
      "|Bodyweight Strength        |200            |Normal   |30          |Moderate |\n",
      "|Cycling (Easy)             |180            |Normal   |30          |Moderate |\n",
      "|HIIT Circuit               |300            |Energetic|20          |High     |\n",
      "|Running / Jogging          |350            |Energetic|30          |High     |\n",
      "|Strength Training (Weights)|400            |Energetic|40          |High     |\n",
      "|Spin Class / Cardio        |380            |Energetic|30          |High     |\n",
      "+---------------------------+---------------+---------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "activity_data = [\n",
    "    # --- Recovery / Tired states ---\n",
    "    {\"Activity\": \"Rest\", \"Category\": \"Tired\", \"Duration_min\": 30, \"Calories_Burned\": 0, \"Intensity\": \"Very Low\"},\n",
    "    {\"Activity\": \"Gentle Yoga\", \"Category\": \"Tired\", \"Duration_min\": 30, \"Calories_Burned\": 80, \"Intensity\": \"Low\"},\n",
    "    {\"Activity\": \"Light Walk\", \"Category\": \"Recovery\", \"Duration_min\": 20, \"Calories_Burned\": 100, \"Intensity\": \"Low\"},\n",
    "    {\"Activity\": \"Stretching Routine\", \"Category\": \"Recovery\", \"Duration_min\": 15, \"Calories_Burned\": 50, \"Intensity\": \"Low\"},\n",
    "\n",
    "    # --- Normal state ---\n",
    "    {\"Activity\": \"Brisk Walk\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 150, \"Intensity\": \"Moderate\"},\n",
    "    {\"Activity\": \"Bodyweight Strength\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 200, \"Intensity\": \"Moderate\"},\n",
    "    {\"Activity\": \"Cycling (Easy)\", \"Category\": \"Normal\", \"Duration_min\": 30, \"Calories_Burned\": 180, \"Intensity\": \"Moderate\"},\n",
    "\n",
    "    # --- Energetic state ---\n",
    "    {\"Activity\": \"HIIT Circuit\", \"Category\": \"Energetic\", \"Duration_min\": 20, \"Calories_Burned\": 300, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Running / Jogging\", \"Category\": \"Energetic\", \"Duration_min\": 30, \"Calories_Burned\": 350, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Strength Training (Weights)\", \"Category\": \"Energetic\", \"Duration_min\": 40, \"Calories_Burned\": 400, \"Intensity\": \"High\"},\n",
    "    {\"Activity\": \"Spin Class / Cardio\", \"Category\": \"Energetic\", \"Duration_min\": 30, \"Calories_Burned\": 380, \"Intensity\": \"High\"},\n",
    "]\n",
    "\n",
    "activity_df = spark.createDataFrame(activity_data)\n",
    "activity_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561033e-898a-4218-9dd1-bda3b5663687",
   "metadata": {},
   "source": [
    "### meals planner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "528336e0-fe97-4235-8679-0dfc12094577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def adjust_daily_targets_pro(row):\n",
    "    \"\"\"\n",
    "    Professional-grade adaptive target adjustment logic.\n",
    "    Uses fatigue, sleep, hydration, activity, and energy balance to optimize user plans.\n",
    "    \"\"\"\n",
    "    # --- Extract key inputs ---\n",
    "    fatigue = row.fatigue_score if row.fatigue_score is not None else 0.8\n",
    "    sleep = row.sleep_hours\n",
    "    intake = row.calories_intake\n",
    "    burnt = row.total_calories_burnt\n",
    "    water = row.water_ml\n",
    "    weight = row.weight\n",
    "    height = row.height\n",
    "    activity = row.total_steps\n",
    "\n",
    "    # --- Derived metrics ---\n",
    "    energy_balance = (intake - burnt) / max(intake, 1)\n",
    "    hydration_ratio = water / max(row[\"total_Water_Intake (ml)\"], 1)\n",
    "    sleep_deficit = (row[\"sleep_hours\"] - sleep) / max(row[\"sleep_hours\"], 1)\n",
    "    tired = fatigue < 0.7\n",
    "    overreached = fatigue < 0.5 and sleep_deficit > 0.2\n",
    "    energetic = fatigue > 0.9 and sleep > row[\"sleep_hours\"]\n",
    "\n",
    "    # --- Base daily targets ---\n",
    "    targets = {\n",
    "        \"total_Calories (kcal)\": row[\"total_Calories (kcal)\"],\n",
    "        \"total_Protein (g)\": row[\"total_Protein (g)\"],\n",
    "        \"total_Carbohydrates (g)\": row[\"total_Carbohydrates (g)\"],\n",
    "        \"total_Fat (g)\": row[\"total_Fat (g)\"],\n",
    "        \"total_Water_Intake (ml)\": row[\"total_Water_Intake (ml)\"],\n",
    "        \"sleep_hours\": row[\"sleep_hours\"],\n",
    "        \"target_steps\": row[\"target_steps\"],\n",
    "        \"target_calories_burnt\": row[\"target_calories_burnt\"],\n",
    "    }\n",
    "\n",
    "    # --- Adaptive adjustment rules ---\n",
    "    if overreached:\n",
    "        # Deep recovery mode\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 0.95,\n",
    "            \"total_Protein (g)\": 1.15,\n",
    "            \"total_Fat (g)\": 0.9,\n",
    "            \"target_steps\": 0.75,\n",
    "            \"target_calories_burnt\": 0.75,\n",
    "            \"sleep_hours\": +1.0,\n",
    "            \"total_Water_Intake (ml)\": 1.2\n",
    "        }\n",
    "        state = \"Recovery\"\n",
    "\n",
    "    elif tired:\n",
    "        # Light fatigue â†’ promote repair\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 0.9,\n",
    "            \"total_Protein (g)\": 1.1,\n",
    "            \"total_Fat (g)\": 0.85,\n",
    "            \"target_steps\": 0.85,\n",
    "            \"target_calories_burnt\": 0.85,\n",
    "            \"sleep_hours\": +0.5,\n",
    "            \"total_Water_Intake (ml)\": 1.1\n",
    "        }\n",
    "        state = \"Tired\"\n",
    "\n",
    "    elif energetic:\n",
    "        # Strong recovery â†’ allow progression\n",
    "        adj = {\n",
    "            \"total_Calories (kcal)\": 1.1,\n",
    "            \"total_Protein (g)\": 1.05,\n",
    "            \"total_Carbohydrates (g)\": 1.1,\n",
    "            \"target_steps\": 1.15,\n",
    "            \"target_calories_burnt\": 1.15,\n",
    "            \"total_Water_Intake (ml)\": 1.05\n",
    "        }\n",
    "        state = \"Energetic\"\n",
    "\n",
    "    else:\n",
    "        # Normal balance\n",
    "        adj = {k: 1.0 for k in targets}\n",
    "        state = \"Normal\"\n",
    "\n",
    "    # --- Fine-tune hydration & energy ---\n",
    "    # Correct for hydration deficiency or excess\n",
    "    if hydration_ratio < 0.9:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.15\n",
    "    elif hydration_ratio > 1.2:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 0.95\n",
    "\n",
    "    # Adjust energy targets based on balance\n",
    "    if energy_balance < -0.15:  # calorie deficit too large\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.05\n",
    "    elif energy_balance > 0.15:  # surplus too large\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "\n",
    "    # Apply scaling\n",
    "    for k, v in adj.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            if v > 1:\n",
    "                targets[k] *= v\n",
    "            elif v < 1:\n",
    "                targets[k] *= v\n",
    "            elif isinstance(v, (int, float)) and \"+\" in str(v):\n",
    "                targets[k] += v\n",
    "\n",
    "    return targets, state\n",
    "\n",
    "\n",
    "# --- 2. Meal Generation ---\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "\n",
    "def generate_plan_with_activity(row):\n",
    "    \"\"\"\n",
    "    Generate a full daily plan including meals and recommended activity.\n",
    "    Activity is chosen based on user's fatigue/recovery state and explained.\n",
    "    \"\"\"\n",
    "    meals = generate_explainable_plan(row)  # existing meal plan function\n",
    "\n",
    "    # Filter activities matching user's plan_state\n",
    "    activities = [a for a in activity_data.value if a[\"Category\"] == row.plan_state]\n",
    "    \n",
    "    # Fallback to Rest if none found\n",
    "    if activities:\n",
    "        chosen_activity = random.choice(activities)\n",
    "    else:\n",
    "        chosen_activity = {\n",
    "            \"Activity\": \"Rest\",\n",
    "            \"Category\": row.plan_state,\n",
    "            \"Duration_min\": 30,\n",
    "            \"Calories_Burned\": 0,\n",
    "            \"Intensity\": \"Very Low\"\n",
    "        }\n",
    "\n",
    "    # Append activity as a special \"meal/activity\" row\n",
    "    meals.append((\n",
    "        row.user_id,\n",
    "        row.date,\n",
    "        \"Activity\",                      # Treat as a meal type for partitioning\n",
    "        chosen_activity[\"Activity\"],\n",
    "        chosen_activity[\"Category\"],\n",
    "        float(chosen_activity[\"Calories_Burned\"]),  # calories burned\n",
    "        0.0, 0.0, 0.0,                    # no macronutrients\n",
    "        row.plan_state,\n",
    "        f\"Activity assigned based on state {row.plan_state}, duration {chosen_activity.get('Duration_min', 30)} min, intensity {chosen_activity.get('Intensity', 'Low')}\",\n",
    "        float(row.fatigue_score),\n",
    "        float(row.sleep_hours),\n",
    "        float(row.total_steps),\n",
    "        float(row.total_calories_burnt),\n",
    "        float(row.calories_intake),\n",
    "        float(row.total_Water_Intake),\n",
    "        0.0, 0.0\n",
    "    ))\n",
    "\n",
    "    return meals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70510f7c-0ba5-45de-9084-050edf0080af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Final explainable plan schema\n",
    "# ----------------------------------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"meal\", StringType()),\n",
    "    StructField(\"food_item\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"Calories\", DoubleType()),\n",
    "    StructField(\"Protein\", DoubleType()),\n",
    "    StructField(\"Carbs\", DoubleType()),\n",
    "    StructField(\"Fat\", DoubleType()),\n",
    "    StructField(\"plan_state\", StringType()),          # Recovery / Tired / Energetic / Normal\n",
    "    StructField(\"reasoning\", StringType()),           # Human-readable explanation\n",
    "    StructField(\"fatigue_score\", DoubleType()),\n",
    "    StructField(\"sleep_hours\", DoubleType()),\n",
    "    StructField(\"total_steps\", DoubleType()),\n",
    "    StructField(\"total_calories_burnt\", DoubleType()),\n",
    "    StructField(\"calories_intake\", DoubleType()),\n",
    "    StructField(\"total_Water_Intake (ml)\", DoubleType()),\n",
    "    StructField(\"energy_balance\", DoubleType()),\n",
    "    StructField(\"hydration_ratio\", DoubleType())\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Adaptive scaling logic with explanation\n",
    "# ----------------------------------------------------\n",
    "def adjust_daily_targets_explainable(row):\n",
    "    fatigue = row.fatigue_score if row.fatigue_score is not None else 0.8\n",
    "    sleep = row.sleep_hours\n",
    "    intake = row.calories_intake\n",
    "    burnt = row.total_calories_burnt\n",
    "    water = row.water_ml\n",
    "    activity = row.total_steps\n",
    "\n",
    "    # Derived metrics\n",
    "    energy_balance = (intake - burnt) / max(intake, 1)\n",
    "    hydration_ratio = water / max(row[\"total_Water_Intake (ml)\"], 1)\n",
    "    sleep_deficit = (row[\"sleep_hours\"] - sleep) / max(row[\"sleep_hours\"], 1)\n",
    "\n",
    "    tired = fatigue < 0.7\n",
    "    overreached = fatigue < 0.5 and sleep_deficit > 0.2\n",
    "    energetic = fatigue > 0.9 and sleep > row[\"sleep_hours\"]\n",
    "\n",
    "    targets = {\n",
    "        \"total_Calories (kcal)\": row[\"total_Calories (kcal)\"],\n",
    "        \"total_Protein (g)\": row[\"total_Protein (g)\"],\n",
    "        \"total_Carbohydrates (g)\": row[\"total_Carbohydrates (g)\"],\n",
    "        \"total_Fat (g)\": row[\"total_Fat (g)\"],\n",
    "        \"total_Water_Intake (ml)\": row[\"total_Water_Intake (ml)\"],\n",
    "        \"sleep_hours\": row[\"sleep_hours\"],\n",
    "        \"target_steps\": row[\"target_steps\"],\n",
    "        \"target_calories_burnt\": row[\"target_calories_burnt\"],\n",
    "    }\n",
    "\n",
    "    reasoning = []\n",
    "\n",
    "    # --- Adaptive adjustments ---\n",
    "    if overreached:\n",
    "        state = \"Recovery\"\n",
    "        reasoning.append(\"Detected signs of overreaching: low fatigue score and sleep deficit.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "        targets[\"total_Protein (g)\"] *= 1.15\n",
    "        targets[\"total_Fat (g)\"] *= 0.9\n",
    "        targets[\"target_steps\"] *= 0.75\n",
    "        targets[\"target_calories_burnt\"] *= 0.75\n",
    "        targets[\"sleep_hours\"] += 1.0\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.2\n",
    "\n",
    "    elif tired:\n",
    "        state = \"Tired\"\n",
    "        reasoning.append(\"Mild fatigue detected â€” focusing on recovery and protein repair.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.9\n",
    "        targets[\"total_Protein (g)\"] *= 1.1\n",
    "        targets[\"total_Fat (g)\"] *= 0.85\n",
    "        targets[\"target_steps\"] *= 0.85\n",
    "        targets[\"target_calories_burnt\"] *= 0.85\n",
    "        targets[\"sleep_hours\"] += 0.5\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.1\n",
    "\n",
    "    elif energetic:\n",
    "        state = \"Energetic\"\n",
    "        reasoning.append(\"High recovery â€” increasing load slightly for progression.\")\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.1\n",
    "        targets[\"total_Protein (g)\"] *= 1.05\n",
    "        targets[\"total_Carbohydrates (g)\"] *= 1.1\n",
    "        targets[\"target_steps\"] *= 1.15\n",
    "        targets[\"target_calories_burnt\"] *= 1.15\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.05\n",
    "\n",
    "    else:\n",
    "        state = \"Normal\"\n",
    "        reasoning.append(\"Balanced state â€” maintaining plan at normal levels.\")\n",
    "\n",
    "    # --- Fine-tune based on energy and hydration ---\n",
    "    if energy_balance < -0.15:\n",
    "        targets[\"total_Calories (kcal)\"] *= 1.05\n",
    "        reasoning.append(\"Caloric deficit detected â€” slightly increasing intake.\")\n",
    "    elif energy_balance > 0.15:\n",
    "        targets[\"total_Calories (kcal)\"] *= 0.95\n",
    "        reasoning.append(\"Caloric surplus detected â€” moderating intake.\")\n",
    "\n",
    "    if hydration_ratio < 0.9:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 1.15\n",
    "        reasoning.append(\"Low hydration â€” increasing water intake.\")\n",
    "    elif hydration_ratio > 1.2:\n",
    "        targets[\"total_Water_Intake (ml)\"] *= 0.95\n",
    "        reasoning.append(\"High hydration â€” lowering water target slightly.\")\n",
    "\n",
    "    return targets, state, \"; \".join(reasoning), energy_balance, hydration_ratio\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Adaptive meal plan generator\n",
    "# ----------------------------------------------------\n",
    "meal_structure = {\n",
    "    \"Breakfast\": [\"Grains\", \"Dairy\", \"Fruits\", \"Beverages\"],\n",
    "    \"Lunch\": [\"Meat\", \"Vegetables\", \"Grains\"],\n",
    "    \"Snack\": [\"Snacks\", \"Fruits\", \"Beverages\"],\n",
    "    \"Dinner\": [\"Meat\", \"Vegetables\", \"Grains\", \"Dairy\"]\n",
    "}\n",
    "def generate_explainable_plan(row):\n",
    "    \"\"\"\n",
    "    Generate an explainable adaptive plan using correct food_df column names.\n",
    "    \"\"\"\n",
    "    global food_data\n",
    "    targets, state, reasoning, energy_balance, hydration_ratio = adjust_daily_targets_explainable(row)\n",
    "    meals = []\n",
    "\n",
    "    for meal, categories in meal_structure.items():\n",
    "        if state in [\"Tired\", \"Recovery\"] and meal in [\"Lunch\", \"Dinner\"]:\n",
    "            categories = [c for c in categories if c not in [\"Snacks\", \"Fried\", \"HighFat\"]]\n",
    "\n",
    "        allowed = [f for f in food_data.value if f[\"Category\"] in categories]\n",
    "        chosen = random.sample(allowed, min(3, len(allowed)))\n",
    "\n",
    "        for f in chosen:\n",
    "            meals.append((\n",
    "                row.user_id,\n",
    "                row.date,\n",
    "                meal,\n",
    "                f[\"Food_Item\"],\n",
    "                f[\"Category\"],\n",
    "                float(f[\"Calories (kcal)\"]),\n",
    "                float(f[\"Protein (g)\"]),\n",
    "                float(f[\"Carbohydrates (g)\"]),\n",
    "                float(f[\"Fat (g)\"]),\n",
    "                state,\n",
    "                reasoning,\n",
    "                float(row.fatigue_score),\n",
    "                float(row.sleep_hours),\n",
    "                float(row.total_steps),\n",
    "                float(row.total_calories_burnt),\n",
    "                float(row.calories_intake),\n",
    "                float(targets[\"total_Water_Intake (ml)\"]),\n",
    "                float(energy_balance),\n",
    "                float(hydration_ratio)\n",
    "            ))\n",
    "    return meals\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Merge daily targets and fatigue data\n",
    "# ----------------------------------------------------\n",
    "daily_df = df_plans.join(fatigue_df, on=[\"user_id\", \"date\"], how=\"left\")\n",
    "food_data = spark.sparkContext.broadcast(food_df.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3df56-3865-4719-a3e8-8684986aed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:>                                                        (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# 5. Generate adaptive explainable plans\n",
    "# ----------------------------------------------------\n",
    "all_plans = (\n",
    "    daily_df.rdd.flatMap(generate_explainable_plan)\n",
    "    .toDF(schema)\n",
    ")\n",
    "# ----------------------------------------------------\n",
    "# 6. User summaries\n",
    "# ----------------------------------------------------\n",
    "summary_df = (\n",
    "    all_plans.groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.sum(\"Calories\").alias(\"TotalCalories\"),\n",
    "        F.sum(\"Protein\").alias(\"TotalProtein\"),\n",
    "        F.sum(\"Carbs\").alias(\"TotalCarbs\"),\n",
    "        F.sum(\"Fat\").alias(\"TotalFat\"),\n",
    "        F.first(\"plan_state\").alias(\"State\"),\n",
    "        F.first(\"reasoning\").alias(\"Explanation\")\n",
    "    )\n",
    ")\n",
    "summary_df.show(10)\n",
    "summary_df.columns\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
